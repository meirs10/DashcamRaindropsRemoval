{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Inference Benchmark & Performance Tuning\n",
                "\n",
                "## Overview\n",
                "This notebook evaluates the runtime performance of the model, focusing on latency and FPS rather than image quality metrics. It helps optimize the deployment configuration (e.g., FP32 vs. FP16/AMP).\n",
                "\n",
                "## Key Capabilities\n",
                "- **Capability Detection**: Automatically detects GPU compute capability and determines if Tensor Cores (Fast FP16) are available.\n",
                "- **Dynamic Benchmarking**: Runs a quick warm-up on sample tiles to measuring if enabling AMP (Automatic Mixed Precision) yields a speedup: \n",
                "    - *If Yes*: Uses FP16 for the full inference loop.\n",
                "    - *If No* (or if CPU): Defaults to FP32 for maximum precision.\n",
                "- **Video Generation**: Creates a side-by-side comparison video (Rainy | Restored | Clean) with real-time latency overlays.\n",
                "- **Detailed Report**: Saves a JSON summary with average, median, and max latency stats."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import sys\n",
                "import time\n",
                "from pathlib import Path\n",
                "from statistics import mean, median\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "import torch\n",
                "from torch.amp import autocast\n",
                "\n",
                "# Set Project Root\n",
                "current_dir = Path.cwd()\n",
                "if current_dir.name == 'testing':\n",
                "    BASE = current_dir.parent\n",
                "else:\n",
                "    BASE = current_dir\n",
                "\n",
                "sys.path.insert(0, str(BASE))\n",
                "print(f'Project Root: {BASE}')\n",
                "\n",
                "from training.helpers.model import MobileNetV3UNetConvLSTMVideo  # type: ignore"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration\n",
                "\n",
                "- **`TARGET_FPS`**: 33.0 (Targeting ~30ms latency for real-time applications).\n",
                "- **`ROWS/COLS`**: 3x5 tiling grid (same as testing pipeline).\n",
                "- **`VIDEO_FPS`**: 10 (Output video playback speed)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --------------------------------------------------------------------\n",
                "# Paths / config\n",
                "# --------------------------------------------------------------------\n",
                "CHECKPOINT_PATH = BASE / \"training\" / \"checkpoints\" / \"stage2\" / \"best_stage2.pth\"\n",
                "\n",
                "RAINY_DIR = BASE / \"data\" / \"data_crapified_test\" / \"scene_004\" / \"front-forward\"\n",
                "CLEAN_DIR = BASE / \"data\" / \"data_original\" / \"scene_004\" / \"images\" / \"front-forward\"\n",
                "\n",
                "OUTPUT_DIR = BASE / \"test_results\" / \"scene_004_inference_benchmark\"\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Target \"real-time\" requirements (for reporting only)\n",
                "TARGET_FPS = 33.0\n",
                "TARGET_MS_PER_FRAME = 1000.0 / TARGET_FPS  # ~30.3 ms\n",
                "\n",
                "# Video output FPS for visualization (unrelated to real-time timing)\n",
                "VIDEO_FPS = 10\n",
                "\n",
                "# Tiling parameters\n",
                "TILE = 512\n",
                "ROWS, COLS = 3, 5  # 3 x 5 = 15 tiles\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Auto-Tuning Functions\n",
                "\n",
                "- **`detect_gpu_features`**: Checks capability version (needs >7.0 for Tensor Cores).\n",
                "- **`benchmark_forward`**: Runs the model 6 times in both FP32 and FP16 modes. Returns `True` (enabled) for AMP only if it is at least 10% faster."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def detect_gpu_features():\n",
                "    \"\"\"\n",
                "    Returns (has_fast_fp16, gpu_name, cc_major, cc_minor) based on *capability*.\n",
                "\n",
                "    Heuristic:\n",
                "      - has_fast_fp16 = True if compute capability >= 7.0\n",
                "        (Volta/Turing/Ampere/Ada – usually RTX / Turing+ GPUs).\n",
                "      - otherwise False (older GTX etc. or CPU).\n",
                "    \"\"\"\n",
                "    if device.type != \"cuda\":\n",
                "        print(\"Running on CPU – using FP32 only.\")\n",
                "        return False, \"CPU\", 0, 0\n",
                "\n",
                "    name = torch.cuda.get_device_name(0)\n",
                "    major, minor = torch.cuda.get_device_capability(0)\n",
                "    has_fast_fp16 = major >= 7  # 7.x and 8.x: Tensor Cores available\n",
                "\n",
                "    print(f\"CUDA GPU: {name} (cc {major}.{minor}), fast_fp16_capability={has_fast_fp16}\")\n",
                "    return has_fast_fp16, name, major, minor\n",
                "\n",
                "\n",
                "HAS_FAST_FP16_CAP, GPU_NAME, CC_MAJOR, CC_MINOR = detect_gpu_features()\n",
                "\n",
                "def benchmark_forward(model: torch.nn.Module, tiles_bchw: torch.Tensor) -> bool:\n",
                "    \"\"\"\n",
                "    Compare FP32 vs FP16 (autocast) on a sample tiles batch and\n",
                "    return USE_AMP = True if FP16 is significantly faster.\n",
                "    \"\"\"\n",
                "    if device.type != \"cuda\" or not HAS_FAST_FP16_CAP:\n",
                "        print(\"Benchmark: no CUDA / no fast FP16 capability -> using FP32.\")\n",
                "        return False\n",
                "\n",
                "    def time_mode(use_amp: bool) -> float:\n",
                "        times = []\n",
                "        for it in range(6):\n",
                "            if device.type == \"cuda\":\n",
                "                torch.cuda.synchronize()\n",
                "            t0 = time.perf_counter()\n",
                "            with torch.no_grad():\n",
                "                if use_amp:\n",
                "                    with autocast(\"cuda\"):\n",
                "                        _ = model(tiles_bchw.unsqueeze(1))  # (N,1,3,H,W)\n",
                "                else:\n",
                "                    _ = model(tiles_bchw.unsqueeze(1))\n",
                "            if device.type == \"cuda\":\n",
                "                torch.cuda.synchronize()\n",
                "            t1 = time.perf_counter()\n",
                "            if it > 0:  # skip first as warm-up\n",
                "                times.append((t1 - t0) * 1000.0)\n",
                "        return float(mean(times)) if times else float(\"inf\")\n",
                "\n",
                "    print(\"\\nBenchmarking FP32 vs FP16 (autocast) on sample tiles...\")\n",
                "    t_fp32 = time_mode(use_amp=False)\n",
                "    t_fp16 = time_mode(use_amp=True)\n",
                "\n",
                "    print(f\"  FP32 avg: {t_fp32:.2f} ms\")\n",
                "    print(f\"  FP16 avg: {t_fp16:.2f} ms\")\n",
                "\n",
                "    # Only enable AMP if it's clearly faster (at least 10% gain)\n",
                "    if t_fp16 < 0.9 * t_fp32:\n",
                "        print(\"=> Using FP16 (autocast) for inference.\\n\")\n",
                "        return True\n",
                "    else:\n",
                "        print(\"=> FP16 not beneficial -> using FP32.\\n\")\n",
                "        return False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualization Helpers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_frame_fullres(path: Path) -> torch.Tensor:\n",
                "    \"\"\"Load image as float32 RGB in [0,1], shape (C,H,W).\"\"\"\n",
                "    img = cv2.imread(str(path))\n",
                "    if img is None:\n",
                "        raise ValueError(f\"Failed to read image: {path}\")\n",
                "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "    img = img.astype(np.float32) / 255.0\n",
                "    return torch.from_numpy(img).permute(2, 0, 1)  # (C,H,W)\n",
                "\n",
                "\n",
                "def get_tile_coords(h: int, w: int, tile: int = 512, rows: int = 3, cols: int = 4):\n",
                "    \"\"\"\n",
                "    Overlapping rows x cols grid, covers full image without padding.\n",
                "    \"\"\"\n",
                "    if h < tile or w < tile:\n",
                "        raise ValueError(f\"Frame too small for tile={tile}: got {w}x{h}\")\n",
                "\n",
                "    xs = np.linspace(0, w - tile, cols).round().astype(int).tolist()\n",
                "    ys = np.linspace(0, h - tile, rows).round().astype(int).tolist()\n",
                "\n",
                "    coords = []\n",
                "    for y in ys:\n",
                "        for x in xs:\n",
                "            coords.append((y, x))\n",
                "    return coords\n",
                "\n",
                "\n",
                "def make_hann_mask(tile: int, device, dtype, eps: float = 1e-6) -> torch.Tensor:\n",
                "    \"\"\"(1, tile, tile) weight mask: high at center, low near edges.\"\"\"\n",
                "    w1 = torch.hann_window(tile, periodic=False, device=device, dtype=dtype)\n",
                "    w2 = torch.outer(w1, w1).clamp_min(eps)  # avoid exact zeros\n",
                "    return w2.unsqueeze(0)  # (1,tile,tile)\n",
                "\n",
                "\n",
                "def tiles_to_full_weighted(\n",
                "    tiles: torch.Tensor,\n",
                "    coords,\n",
                "    h: int,\n",
                "    w: int,\n",
                "    mask: torch.Tensor,\n",
                "    tile: int = 512,\n",
                ") -> torch.Tensor:\n",
                "    \"\"\"\n",
                "    Weighted overlap-add.\n",
                "    \"\"\"\n",
                "    acc = torch.zeros((3, h, w), device=tiles.device, dtype=tiles.dtype)\n",
                "    wgt = torch.zeros((1, h, w), device=tiles.device, dtype=tiles.dtype)\n",
                "\n",
                "    for i, (y0, x0) in enumerate(coords):\n",
                "        acc[:, y0:y0 + tile, x0:x0 + tile] += tiles[i] * mask\n",
                "        wgt[:, y0:y0 + tile, x0:x0 + tile] += mask\n",
                "\n",
                "    return acc / wgt.clamp_min(1e-6)\n",
                "\n",
                "\n",
                "def chw_to_bgr_uint8(chw: torch.Tensor) -> np.ndarray:\n",
                "    chw = chw.clamp(0, 1)\n",
                "    hwc = chw.permute(1, 2, 0).cpu().numpy()\n",
                "    hwc_u8 = (hwc * 255.0 + 0.5).astype(np.uint8)\n",
                "    return cv2.cvtColor(hwc_u8, cv2.COLOR_RGB2BGR)\n",
                "\n",
                "\n",
                "def stack_triplet_vertical(\n",
                "    rainy_bgr: np.ndarray,\n",
                "    out_bgr: np.ndarray,\n",
                "    clean_bgr: np.ndarray,\n",
                ") -> np.ndarray:\n",
                "    return np.concatenate([rainy_bgr, out_bgr, clean_bgr], axis=0)\n",
                "\n",
                "\n",
                "def put_labels(\n",
                "    stacked_bgr: np.ndarray,\n",
                "    w: int,\n",
                "    h: int,\n",
                "    dt_ms: float,\n",
                "    inst_fps: float,\n",
                ") -> np.ndarray:\n",
                "    img = stacked_bgr\n",
                "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
                "    scale = 0.9\n",
                "    thickness = 2\n",
                "\n",
                "    cv2.putText(img, \"RAINY INPUT\",   (20, 40),\n",
                "                font, scale, (0, 0, 255), thickness)\n",
                "    cv2.putText(img, \"MODEL OUTPUT\",  (20, h + 40),\n",
                "                font, scale, (0, 255, 0), thickness)\n",
                "    cv2.putText(img, \"GROUND TRUTH\",  (20, 2 * h + 40),\n",
                "                font, scale, (255, 0, 0), thickness)\n",
                "    \n",
                "    # Overlays for timing\n",
                "    txt = f\"{dt_ms:.1f} ms  |  {inst_fps:.1f} FPS\"\n",
                "    cv2.putText(\n",
                "        img,\n",
                "        txt,\n",
                "        (w - 320, 40),\n",
                "        font,\n",
                "        scale,\n",
                "        (255, 255, 255),\n",
                "        thickness,\n",
                "    )\n",
                "    return img\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Main Execution\n",
                "\n",
                "1.  **Warm Up**: Runs a few batches to wake up the GPU.\n",
                "2.  **Benchmark Loop**: Processes frames, measuring ONLY the inference time (including data transfer to GPU) that impacts latency.\n",
                "3.  **Visualization**: Saves the processed video for qualitative review."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nLoading Stage-2 checkpoint...\")\n",
                "if not CHECKPOINT_PATH.exists():\n",
                "    raise FileNotFoundError(f\"Checkpoint not found: {CHECKPOINT_PATH}\")\n",
                "\n",
                "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
                "\n",
                "model = MobileNetV3UNetConvLSTMVideo(\n",
                "    hidden_dim=96,\n",
                "    out_channels=3,\n",
                "    use_pretrained_encoder=True,\n",
                "    freeze_encoder=True,\n",
                ").to(device)\n",
                "\n",
                "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
                "model.eval()\n",
                "\n",
                "print(\n",
                "    f\"✓ Loaded checkpoint from epoch {checkpoint['epoch']} \"\n",
                "    f\"(val_loss={checkpoint['val_loss']:.6f})\\n\"\n",
                ")\n",
                "\n",
                "rainy_files = sorted(RAINY_DIR.glob(\"*.jpeg\"))\n",
                "clean_files = sorted(CLEAN_DIR.glob(\"*.jpeg\"))\n",
                "n = min(len(rainy_files), len(clean_files))\n",
                "rainy_files, clean_files = rainy_files[:n], clean_files[:n]\n",
                "if n == 0:\n",
                "    raise RuntimeError(\"No frames found in the specified directories.\")\n",
                "\n",
                "print(f\"Using {n} frames from:\")\n",
                "print(f\"  Rainy:  {RAINY_DIR}\")\n",
                "print(f\"  Clean:  {CLEAN_DIR}\\n\")\n",
                "\n",
                "# Probe resolution from first frame\n",
                "rainy0 = load_frame_fullres(rainy_files[0])\n",
                "_, h, w = rainy0.shape\n",
                "\n",
                "coords = get_tile_coords(h, w, tile=TILE, rows=ROWS, cols=COLS)\n",
                "n_tiles = len(coords)\n",
                "print(f\"Tiling: {ROWS} x {COLS} => {n_tiles} tiles per frame (tile={TILE})\")\n",
                "assert n_tiles == ROWS * COLS, f\"Expected {ROWS*COLS} tiles, got {n_tiles}\"\n",
                "\n",
                "# Build sample tiles batch (for AMP benchmark)\n",
                "sample_tiles = []\n",
                "for (y0, x0) in coords:\n",
                "    sample_tiles.append(rainy0[:, y0:y0 + TILE, x0:x0 + TILE])\n",
                "sample_tiles = torch.stack(sample_tiles, dim=0).to(device)  # (N,3,TILE,TILE)\n",
                "\n",
                "# Decide whether to use AMP based on actual timing\n",
                "USE_AMP = benchmark_forward(model, sample_tiles)\n",
                "\n",
                "# Precompute Hann mask (fp32) on the correct device\n",
                "hann_mask = make_hann_mask(TILE, device=device, dtype=torch.float32)\n",
                "\n",
                "# Setup video writer\n",
                "video_path = OUTPUT_DIR / \"comparison_stacked_10fps.mp4\"\n",
                "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
                "writer = cv2.VideoWriter(str(video_path), fourcc, VIDEO_FPS, (w, h * 3))\n",
                "if not writer.isOpened():\n",
                "    raise RuntimeError(\"Failed to open VideoWriter (mp4v).\")\n",
                "\n",
                "print(\"Running overlapping + weighted-blend inference (timed)...\")\n",
                "\n",
                "# Timing stats\n",
                "times_ms = []\n",
                "\n",
                "for i, (rp, cp) in enumerate(zip(rainy_files, clean_files)):\n",
                "    rainy_chw = load_frame_fullres(rp)  # CPU\n",
                "    clean_chw = load_frame_fullres(cp)  # CPU\n",
                "\n",
                "    if rainy_chw.shape != clean_chw.shape:\n",
                "        raise ValueError(\n",
                "            f\"Shape mismatch at frame {i}: {rainy_chw.shape} vs {clean_chw.shape}\"\n",
                "        )\n",
                "\n",
                "    # Build tiles on CPU\n",
                "    tiles = []\n",
                "    for (y0, x0) in coords:\n",
                "        tiles.append(rainy_chw[:, y0:y0 + TILE, x0:x0 + TILE])\n",
                "    tiles = torch.stack(tiles, dim=0)  # (N,3,TILE,TILE)\n",
                "\n",
                "    # Move to device\n",
                "    tiles = tiles.to(device, non_blocking=True)\n",
                "\n",
                "    # Inference timing: one batched forward over all tiles\n",
                "    if device.type == \"cuda\":\n",
                "        torch.cuda.synchronize()\n",
                "    t0 = time.perf_counter()\n",
                "\n",
                "    with torch.no_grad():\n",
                "        if USE_AMP and device.type == \"cuda\":\n",
                "            with autocast(\"cuda\"):\n",
                "                inp = tiles.unsqueeze(1)      # (N,1,3,512,512)\n",
                "                out_tiles = model(inp)        # (N,1,3,512,512)\n",
                "        else:\n",
                "            inp = tiles.unsqueeze(1)          # (N,1,3,512,512)\n",
                "            out_tiles = model(inp)\n",
                "\n",
                "    if device.type == \"cuda\":\n",
                "        torch.cuda.synchronize()\n",
                "    t1 = time.perf_counter()\n",
                "\n",
                "    dt_ms = (t1 - t0) * 1000.0\n",
                "    times_ms.append(dt_ms)\n",
                "    inst_fps = 1000.0 / dt_ms if dt_ms > 0 else float(\"inf\")\n",
                "\n",
                "    # Prepare output tiles for blending (fp32)\n",
                "    out_tiles = out_tiles.squeeze(1)  # (N,3,TILE,TILE)\n",
                "    if USE_AMP and device.type == \"cuda\":\n",
                "        out_tiles = out_tiles.to(torch.float32)\n",
                "\n",
                "    out_full = tiles_to_full_weighted(\n",
                "        out_tiles,\n",
                "        coords,\n",
                "        h=h,\n",
                "        w=w,\n",
                "        mask=hann_mask,  # already on correct device\n",
                "        tile=TILE,\n",
                "    ).clamp(0, 1).cpu()\n",
                "\n",
                "    rainy_bgr = chw_to_bgr_uint8(rainy_chw)\n",
                "    out_bgr = chw_to_bgr_uint8(out_full)\n",
                "    clean_bgr = chw_to_bgr_uint8(clean_chw)\n",
                "\n",
                "    stacked = stack_triplet_vertical(rainy_bgr, out_bgr, clean_bgr)\n",
                "    stacked = put_labels(stacked, w=w, h=h, dt_ms=dt_ms, inst_fps=inst_fps)\n",
                "\n",
                "    # Save frame + video\n",
                "    out_path = OUTPUT_DIR / f\"comparison_frame_{i:04d}_stacked.png\"\n",
                "    cv2.imwrite(str(out_path), stacked)\n",
                "    writer.write(stacked)\n",
                "\n",
                "    status = (\n",
                "        f\"Frame {i+1}/{n}: {dt_ms:.2f} ms \"\n",
                "        f\"({inst_fps:.1f} FPS) \"\n",
                "        f\"{'OK' if dt_ms <= TARGET_MS_PER_FRAME else 'SLOW'}\"\n",
                "    )\n",
                "    print(status)\n",
                "\n",
                "writer.release()\n",
                "\n",
                "# ----------------------------------------------------------------\n",
                "# Timing summary\n",
                "# ----------------------------------------------------------------\n",
                "if times_ms:\n",
                "    avg_ms = float(mean(times_ms))\n",
                "    med_ms = float(median(times_ms))\n",
                "    max_ms = float(max(times_ms))\n",
                "    eff_fps_avg = 1000.0 / avg_ms\n",
                "    eff_fps_med = 1000.0 / med_ms\n",
                "\n",
                "    print(\"\\n✓ Inference complete.\")\n",
                "    print(f\"✓ Saved {n} stacked images to: {OUTPUT_DIR}\")\n",
                "    print(f\"✓ Saved video to: {video_path}\")\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\n",
                "        \"Timing (batch of tiles per frame, including model forward only):\"\n",
                "    )\n",
                "    print(f\"  avg latency   : {avg_ms:.2f} ms  -> {eff_fps_avg:.2f} FPS\")\n",
                "    print(f\"  median latency: {med_ms:.2f} ms  -> {eff_fps_med:.2f} FPS\")\n",
                "    print(f\"  max latency   : {max_ms:.2f} ms\")\n",
                "    print(f\"  Real-time target: {TARGET_MS_PER_FRAME:.2f} ms per frame \"\n",
                "          f\"({TARGET_FPS:.1f} FPS)\")\n",
                "\n",
                "    meets_realtime = avg_ms <= TARGET_MS_PER_FRAME\n",
                "    print(f\"\\nMeets 33 FPS on average? {'YES' if meets_realtime else 'NO'}\")\n",
                "    print(f\"Stage-2 best val loss (combined): {checkpoint['val_loss']:.6f}\")\n",
                "    print(\"=\" * 60 + \"\\n\")\n",
                "\n",
                "    # Save summary as JSON\n",
                "    summary = {\n",
                "        \"num_frames\": n,\n",
                "        \"avg_ms_per_frame\": avg_ms,\n",
                "        \"median_ms_per_frame\": med_ms,\n",
                "        \"max_ms_per_frame\": max_ms,\n",
                "        \"effective_fps_avg\": eff_fps_avg,\n",
                "        \"effective_fps_median\": eff_fps_med,\n",
                "        \"target_fps\": TARGET_FPS,\n",
                "        \"target_ms_per_frame\": TARGET_MS_PER_FRAME,\n",
                "        \"meets_realtime_avg\": meets_realtime,\n",
                "        \"checkpoint_epoch\": int(checkpoint[\"epoch\"]),\n",
                "        \"checkpoint_val_loss\": float(checkpoint[\"val_loss\"]),\n",
                "        \"gpu_name\": GPU_NAME,\n",
                "        \"compute_capability\": f\"{CC_MAJOR}.{CC_MINOR}\",\n",
                "        \"has_fast_fp16_capability\": HAS_FAST_FP16_CAP,\n",
                "        \"use_amp\": USE_AMP,\n",
                "    }\n",
                "    summary_path = OUTPUT_DIR / \"inference_timing_summary.json\"\n",
                "    with open(summary_path, \"w\") as f:\n",
                "        json.dump(summary, f, indent=4)\n",
                "    print(f\"Timing summary saved to: {summary_path}\")\n",
                "else:\n",
                "    print(\"No timing data collected (no frames?).\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
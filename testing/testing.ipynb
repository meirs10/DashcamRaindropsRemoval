{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Stage 2 Testing: Tiled Inference & Restoration\n",
                "\n",
                "## Overview\n",
                "This notebook implements the inference pipeline for the **Stage 2** model. \n",
                "\n",
                "### Why Tiling?\n",
                "High-resolution dashcam footage (e.g., 1920x1080) cannot be processed in a single pass due to VRAM limitations and the model's fixed input size (512x512). To address this, we use a **Tiled Inference** strategy:\n",
                "1.  **Split**: The image is divided into overlapping tiles (512x512).\n",
                "2.  **Process**: Each tile is independently restored by the model.\n",
                "3.  **Blend**: The restored tiles are stitched back together using **Weighted Blending** (Hann window) to eliminate visible seams at the tile boundaries.\n",
                "\n",
                "### Metrics\n",
                "The notebook calculates full-frame quality metrics compared to the \"Clean\" ground truth:\n",
                "- **MAE** (Mean Absolute Error)\n",
                "- **MSE** (Mean Squared Error)\n",
                "- **PSNR** (Peak Signal-to-Noise Ratio)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import math\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "\n",
                "# Set Project Root\n",
                "current_dir = Path.cwd()\n",
                "if current_dir.name == 'testing':\n",
                "    BASE = current_dir.parent\n",
                "else:\n",
                "    BASE = current_dir\n",
                "\n",
                "sys.path.insert(0, str(BASE))\n",
                "print(f'Project Root: {BASE}')\n",
                "\n",
                "from training.helpers.model import MobileNetV3UNetConvLSTMVideo\n",
                "from training.helpers.losses import CombinedVideoLoss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration\n",
                "\n",
                "- **`CHECKPOINT_PATH`**: Automatically checks for `best_stage2.pth`.\n",
                "- **`OUTPUT_DIR`**: Results are saved to `test_results/stage2_test_tiled`.\n",
                "- **`Tiling Config`**: \n",
                "    - `TILE = 512`: Size of the square tile.\n",
                "    - `ROWS=3, COLS=5`: Defines a grid covering the image with sufficient overlap."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ------------------------------------------------------------------------------------\n",
                "# Paths and constants\n",
                "# ------------------------------------------------------------------------------------\n",
                "CLEAN_DATA = BASE / \"data\" / \"data_original\"\n",
                "RAINY_DATA = BASE / \"data\" / \"data_crapified_test\"\n",
                "SPLIT_FILE = BASE / \"crapification\" / \"helpers\" / \"scene_split.json\"\n",
                "\n",
                "# Possible checkpoint locations\n",
                "CHECKPOINT_PATH_ROOT = BASE / \"training\" / \"checkpoints\" / \"best_stage2.pth\"\n",
                "CHECKPOINT_PATH_STAGE2 = BASE / \"training\" / \"checkpoints\" / \"stage2\" / \"best_stage2.pth\"\n",
                "\n",
                "OUTPUT_DIR = BASE / \"test_results\" / \"stage2_test_tiled\"\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Tiling parameters – MUST match overlapping_inference.py\n",
                "TILE = 512\n",
                "ROWS, COLS = 3, 5   # 3 x 5 = 15 tiles\n",
                "\n",
                "# Same max weights as train_stage_2.py\n",
                "SSIM_MAX = 0.15\n",
                "EDGE_MAX = 0.10\n",
                "PERCEPTUAL_MAX = 0.05\n",
                "\n",
                "ANGLES = [\n",
                "    \"front-forward\",\n",
                "    \"left-backward\",\n",
                "    \"left-forward\",\n",
                "    \"right-backward\",\n",
                "    \"right-forward\",\n",
                "]\n",
                "\n",
                "# How many visual samples to save\n",
                "SAMPLE_LIMIT = 20\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Functions: Tiling & Reconstruction\n",
                "\n",
                "- **`get_tile_coords`**: Calculates the top-left coordinates for the 3x5 grid.\n",
                "- **`make_hann_mask`**: Generates a 2D Hann window. This mask is crucial; it gives higher weight to the center of the tile and near-zero weight to the edges, ensuring that the final blended image is seamless.\n",
                "- **`tiles_to_full_weighted`**: Reconstructs the full image by placing each tile in its position, multiplying by the Hann mask, and normalizing by the sum of weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_frame_fullres(path: Path) -> torch.Tensor:\n",
                "    \"\"\"Load image as float32 RGB in [0,1], shape (C,H,W).\"\"\"\n",
                "    img = cv2.imread(str(path))\n",
                "    if img is None:\n",
                "        raise ValueError(f\"Failed to read image: {path}\")\n",
                "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "    img = img.astype(np.float32) / 255.0\n",
                "    return torch.from_numpy(img).permute(2, 0, 1)  # (C,H,W)\n",
                "\n",
                "\n",
                "def get_tile_coords(h: int, w: int, tile: int = 512, rows: int = 3, cols: int = 4):\n",
                "    \"\"\"\n",
                "    Overlapping rows x cols grid, covers full image without padding.\n",
                "    Uses rounded linspace so last tile aligns to bottom/right.\n",
                "    \"\"\"\n",
                "    if h < tile or w < tile:\n",
                "        raise ValueError(f\"Frame too small for tile={tile}: got {w}x{h}\")\n",
                "\n",
                "    xs = np.linspace(0, w - tile, cols).round().astype(int).tolist()\n",
                "    ys = np.linspace(0, h - tile, rows).round().astype(int).tolist()\n",
                "\n",
                "    coords = []\n",
                "    for y in ys:\n",
                "        for x in xs:\n",
                "            coords.append((y, x))\n",
                "    return coords\n",
                "\n",
                "\n",
                "def make_hann_mask(tile: int, device, dtype, eps: float = 1e-6) -> torch.Tensor:\n",
                "    \"\"\"(1, tile, tile) weight mask: high at center, low near edges.\"\"\"\n",
                "    w1 = torch.hann_window(tile, periodic=False, device=device, dtype=dtype)\n",
                "    w2 = torch.outer(w1, w1).clamp_min(eps)  # avoid exact zeros\n",
                "    return w2.unsqueeze(0)  # (1,tile,tile)\n",
                "\n",
                "\n",
                "def tiles_to_full_weighted(\n",
                "    tiles: torch.Tensor,\n",
                "    coords,\n",
                "    h: int,\n",
                "    w: int,\n",
                "    tile: int = 512,\n",
                ") -> torch.Tensor:\n",
                "    \"\"\"\n",
                "    Weighted overlap-add.\n",
                "    tiles: (N,3,tile,tile)\n",
                "    \"\"\"\n",
                "    acc = torch.zeros((3, h, w), device=tiles.device, dtype=tiles.dtype)\n",
                "    wgt = torch.zeros((1, h, w), device=tiles.device, dtype=tiles.dtype)\n",
                "\n",
                "    mask = make_hann_mask(tile, device=tiles.device, dtype=tiles.dtype)  # (1,tile,tile)\n",
                "\n",
                "    for i, (y0, x0) in enumerate(coords):\n",
                "        acc[:, y0:y0 + tile, x0:x0 + tile] += tiles[i] * mask\n",
                "        wgt[:, y0:y0 + tile, x0:x0 + tile] += mask\n",
                "\n",
                "    return acc / wgt.clamp_min(1e-6)\n",
                "\n",
                "\n",
                "def chw_to_bgr_uint8(chw: torch.Tensor) -> np.ndarray:\n",
                "    \"\"\"Convert (C,H,W) float [0,1] tensor to OpenCV BGR uint8 image.\"\"\"\n",
                "    chw = chw.clamp(0, 1)\n",
                "    hwc = chw.permute(1, 2, 0).cpu().numpy()\n",
                "    hwc_u8 = (hwc * 255.0 + 0.5).astype(np.uint8)\n",
                "    return cv2.cvtColor(hwc_u8, cv2.COLOR_RGB2BGR)\n",
                "\n",
                "\n",
                "def fullframe_metrics(pred: torch.Tensor, target: torch.Tensor):\n",
                "    \"\"\"\n",
                "    pred, target: (3,H,W), float in [0,1], same device\n",
                "    Returns (mae, mse, psnr)\n",
                "    \"\"\"\n",
                "    diff = pred - target\n",
                "    mae = diff.abs().mean().item()\n",
                "    mse = (diff ** 2).mean().item()\n",
                "    eps = 1e-8\n",
                "    psnr = 10.0 * math.log10(1.0 / (mse + eps))\n",
                "    return mae, mse, psnr"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Main Execution\n",
                "\n",
                "1.  **Setup**: Loads the model and `scene_split.json` to identify test scenes.\n",
                "2.  **Iterate**: Loops through every angle of each test scene.\n",
                "3.  **Process**: \n",
                "    - Loads Clean and Rainy frames.\n",
                "    - Slices them into tiles.\n",
                "    - Runs model inference on the batch of tiles.\n",
                "    - Reconstructs the full frame.\n",
                "    - Computes metrics (PSNR, MAE).\n",
                "4.  **Save**: Dumps a summary JSON and sample visualizations (Rainy | Output | Clean) to `test_results/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def main():\n",
                "    # ---------------- Scene split ----------------\n",
                "    if not SPLIT_FILE.exists():\n",
                "        raise FileNotFoundError(f\"Split file not found: {SPLIT_FILE}\")\n",
                "\n",
                "    with open(SPLIT_FILE, \"r\") as f:\n",
                "        split_info = json.load(f)\n",
                "\n",
                "    test_scenes = split_info.get(\"test\", [])\n",
                "    print(f\"TEST scenes from split file: {len(test_scenes)} -> {test_scenes}\")\n",
                "\n",
                "    # ---------------- Model + checkpoint ----------------\n",
                "    if CHECKPOINT_PATH_ROOT.exists():\n",
                "        ckpt_path = CHECKPOINT_PATH_ROOT\n",
                "    elif CHECKPOINT_PATH_STAGE2.exists():\n",
                "        ckpt_path = CHECKPOINT_PATH_STAGE2\n",
                "    else:\n",
                "        raise FileNotFoundError(\n",
                "            \"Checkpoint not found at either:\\n\"\n",
                "            f\"  {CHECKPOINT_PATH_ROOT}\\n\"\n",
                "            f\"  {CHECKPOINT_PATH_STAGE2}\"\n",
                "        )\n",
                "\n",
                "    print(f\"\\nLoading checkpoint from: {ckpt_path}\")\n",
                "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
                "\n",
                "    model = MobileNetV3UNetConvLSTMVideo(\n",
                "        hidden_dim=96,\n",
                "        out_channels=3,\n",
                "        use_pretrained_encoder=True,\n",
                "        freeze_encoder=True,\n",
                "    ).to(device)\n",
                "\n",
                "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
                "    model.eval()\n",
                "\n",
                "    best_val_loss = checkpoint.get(\"val_loss\", float(\"nan\"))\n",
                "\n",
                "    print(\n",
                "        f\"✓ Loaded epoch {checkpoint.get('epoch', 'N/A')} \"\n",
                "        f\"(val_loss={best_val_loss:.6f})\\n\"\n",
                "    )\n",
                "\n",
                "    # Same CombinedVideoLoss as Stage 2, but with final max weights\n",
                "    criterion = CombinedVideoLoss(\n",
                "        alpha=1.0,\n",
                "        beta=SSIM_MAX,\n",
                "        gamma=EDGE_MAX,\n",
                "        delta=0.0,\n",
                "        epsilon=PERCEPTUAL_MAX,\n",
                "    ).to(device)\n",
                "\n",
                "    print(\"Using CombinedVideoLoss for tiles with final max weights:\")\n",
                "    print(f\"  alpha (pixel)   = 1.0\")\n",
                "    print(f\"  beta  (SSIM)    = {SSIM_MAX}\")\n",
                "    print(f\"  gamma (Edge)    = {EDGE_MAX}\")\n",
                "    print(f\"  epsilon (Perc.) = {PERCEPTUAL_MAX}\")\n",
                "    print(f\"  delta (Temporal)= 0.0\\n\")\n",
                "\n",
                "    # ---------------- Global accumulators ----------------\n",
                "    total_frames = 0\n",
                "    sum_mae = 0.0\n",
                "    sum_mse = 0.0\n",
                "    sum_psnr = 0.0\n",
                "\n",
                "    sum_total_max = 0.0   # sum over tiles\n",
                "    total_tiles = 0       # how many tiles we evaluated\n",
                "\n",
                "    sample_count = 0      # how many qualitative examples saved\n",
                "\n",
                "    # ---------------- Iterate over test scenes and angles ----------------\n",
                "    for scene_num in test_scenes:\n",
                "        scene_name = f\"scene_{scene_num:03d}\"\n",
                "        print(f\"\\n=== Scene {scene_name} ===\")\n",
                "\n",
                "        for angle in ANGLES:\n",
                "            clean_dir = CLEAN_DATA / scene_name / \"images\" / angle\n",
                "            rainy_dir = RAINY_DATA / scene_name / angle\n",
                "\n",
                "            if not clean_dir.exists() or not rainy_dir.exists():\n",
                "                # no such angle for this scene in test data_original\n",
                "                continue\n",
                "\n",
                "            clean_files = sorted(clean_dir.glob(\"*.jpeg\"))\n",
                "            rainy_files = sorted(rainy_dir.glob(\"*.jpeg\"))\n",
                "            n = min(len(clean_files), len(rainy_files))\n",
                "            if n == 0:\n",
                "                continue\n",
                "\n",
                "            print(f\"  Angle {angle}: {n} frames\")\n",
                "\n",
                "            for i in range(n):\n",
                "                cp = clean_files[i]\n",
                "                rp = rainy_files[i]\n",
                "\n",
                "                # ----- Load full-res frames (CPU) -----\n",
                "                clean_chw = load_frame_fullres(cp)   # (3,H,W), float in [0,1]\n",
                "                rainy_chw = load_frame_fullres(rp)\n",
                "\n",
                "                if clean_chw.shape != rainy_chw.shape:\n",
                "                    raise ValueError(\n",
                "                        f\"Shape mismatch at {scene_name} {angle} frame {i}: \"\n",
                "                        f\"{clean_chw.shape} vs {rainy_chw.shape}\"\n",
                "                    )\n",
                "\n",
                "                _, H, W = rainy_chw.shape\n",
                "\n",
                "                # ----- Build tiles exactly as in overlapping_inference.py -----\n",
                "                coords = get_tile_coords(H, W, tile=TILE, rows=ROWS, cols=COLS)\n",
                "                tiles_rainy = []\n",
                "                tiles_clean = []\n",
                "                for (y0, x0) in coords:\n",
                "                    tiles_rainy.append(rainy_chw[:, y0:y0 + TILE, x0:x0 + TILE])\n",
                "                    tiles_clean.append(clean_chw[:, y0:y0 + TILE, x0:x0 + TILE])\n",
                "\n",
                "                tiles_rainy = torch.stack(tiles_rainy, dim=0)  # (N,3,512,512)\n",
                "                tiles_clean = torch.stack(tiles_clean, dim=0)  # (N,3,512,512)\n",
                "                N_tiles = tiles_rainy.size(0)\n",
                "\n",
                "                # ----- Forward through model: (N,1,3,512,512) -----\n",
                "                inp = tiles_rainy.unsqueeze(1).to(device)   # (N,1,3,512,512)\n",
                "                target_tiles = tiles_clean.unsqueeze(1).to(device)\n",
                "\n",
                "                with torch.no_grad():\n",
                "                    out_tiles = model(inp)                 # (N,1,3,512,512)\n",
                "\n",
                "                out_tiles = out_tiles.squeeze(1)           # (N,3,512,512)\n",
                "\n",
                "                # ----- CombinedVideoLoss on tiles (training-style) -----\n",
                "                # Criterion expects (B,T,C,H,W), so we add back T=1:\n",
                "                out_seq = out_tiles.unsqueeze(1)           # (N,1,3,512,512)\n",
                "\n",
                "                loss, loss_dict = criterion(out_seq, target_tiles)\n",
                "\n",
                "                pixel_loss = loss_dict[\"pixel\"]\n",
                "                ssim_loss = loss_dict[\"ssim\"]\n",
                "                edge_loss = loss_dict[\"edge\"]\n",
                "                perc_loss = loss_dict[\"perceptual\"]\n",
                "\n",
                "                total_max_tensor = (\n",
                "                    1.0 * pixel_loss\n",
                "                    + SSIM_MAX * ssim_loss\n",
                "                    + EDGE_MAX * edge_loss\n",
                "                    + PERCEPTUAL_MAX * perc_loss\n",
                "                )\n",
                "                total_max = float(total_max_tensor)\n",
                "\n",
                "                sum_total_max += total_max * N_tiles\n",
                "                total_tiles += N_tiles\n",
                "\n",
                "                # ----- Reconstruct full-res frame via Hann blending -----\n",
                "                out_full = tiles_to_full_weighted(\n",
                "                    out_tiles,\n",
                "                    coords,\n",
                "                    h=H,\n",
                "                    w=W,\n",
                "                    tile=TILE,\n",
                "                ).clamp(0, 1)\n",
                "\n",
                "                # ----- Full-frame metrics on CPU -----\n",
                "                out_full_cpu = out_full.cpu()\n",
                "                clean_cpu = clean_chw\n",
                "\n",
                "                mae_frame, mse_frame, psnr_frame = fullframe_metrics(\n",
                "                    out_full_cpu, clean_cpu\n",
                "                )\n",
                "\n",
                "                total_frames += 1\n",
                "                sum_mae += mae_frame\n",
                "                sum_mse += mse_frame\n",
                "                sum_psnr += psnr_frame\n",
                "\n",
                "                # ----- Save a few qualitative samples -----\n",
                "                if sample_count < SAMPLE_LIMIT:\n",
                "                    rainy_bgr = chw_to_bgr_uint8(rainy_chw)\n",
                "                    out_bgr = chw_to_bgr_uint8(out_full_cpu)\n",
                "                    clean_bgr = chw_to_bgr_uint8(clean_cpu)\n",
                "\n",
                "                    stacked = np.concatenate([rainy_bgr, out_bgr, clean_bgr], axis=1)\n",
                "                    out_path = (\n",
                "                        OUTPUT_DIR\n",
                "                        / f\"sample_{scene_name}_{angle}_frame{i:04d}.png\"\n",
                "                    )\n",
                "                    cv2.imwrite(str(out_path), stacked)\n",
                "                    sample_count += 1\n",
                "\n",
                "                # Some light logging\n",
                "                if (total_frames % 100) == 0:\n",
                "                    print(\n",
                "                        f\"    Frame {total_frames}: \"\n",
                "                        f\"MAE={mae_frame:.4f}, PSNR={psnr_frame:.2f} dB, \"\n",
                "                        f\"tile_loss={total_max:.4f}\"\n",
                "                    )\n",
                "\n",
                "    if total_frames == 0:\n",
                "        print(\"No frames processed. Check your test split and data_original paths.\")\n",
                "        return\n",
                "\n",
                "    mean_mae = sum_mae / total_frames\n",
                "    mean_mse = sum_mse / total_frames\n",
                "    mean_psnr = sum_psnr / total_frames\n",
                "    mean_total_max = sum_total_max / total_tiles if total_tiles > 0 else float(\"nan\")\n",
                "\n",
                "    print(\"\\n\" + \"=\" * 70)\n",
                "    print(\"Stage 2 TILED TEST results (same pipeline as overlapping_inference.py):\")\n",
                "    print(f\"  Num frames                : {total_frames}\")\n",
                "    print(f\"  Num tiles (all frames)    : {total_tiles}\")\n",
                "    print(f\"  Train-style TEST loss     : {mean_total_max:.6f}\")\n",
                "    print(f\"  Best VAL loss (checkpoint): {best_val_loss:.6f}\")\n",
                "    print(f\"  Mean MAE (full-frame)     : {mean_mae:.6f}\")\n",
                "    print(f\"  Mean MSE (full-frame)     : {mean_mse:.6f}\")\n",
                "    print(f\"  Mean PSNR (full-frame)    : {mean_psnr:.2f} dB\")\n",
                "    print(\"=\" * 70)\n",
                "\n",
                "    # ---------------- Save metrics to files ----------------\n",
                "    summary = {\n",
                "        \"num_frames\": total_frames,\n",
                "        \"num_tiles\": total_tiles,\n",
                "        \"test_loss_combined\": mean_total_max,\n",
                "        \"best_val_loss\": best_val_loss,\n",
                "        \"mean_mae\": mean_mae,\n",
                "        \"mean_mse\": mean_mse,\n",
                "        \"mean_psnr_db\": mean_psnr,\n",
                "    }\n",
                "\n",
                "    summary_json_path = OUTPUT_DIR / \"metrics_summary.json\"\n",
                "    with open(summary_json_path, \"w\") as f:\n",
                "        json.dump(summary, f, indent=4)\n",
                "\n",
                "    summary_txt_path = OUTPUT_DIR / \"metrics_summary.txt\"\n",
                "    with open(summary_txt_path, \"w\") as f:\n",
                "        f.write(\"Stage 2 TILED TEST results (same pipeline as overlapping_inference.py)\\n\")\n",
                "        f.write(f\"Num frames             : {total_frames}\\n\")\n",
                "        f.write(f\"Num tiles              : {total_tiles}\\n\")\n",
                "        f.write(f\"Train-style TEST loss  : {mean_total_max:.6f}\\n\")\n",
                "        f.write(f\"Best VAL loss          : {best_val_loss:.6f}\\n\")\n",
                "        f.write(f\"Mean MAE               : {mean_mae:.6f}\\n\")\n",
                "        f.write(f\"Mean MSE               : {mean_mse:.6f}\\n\")\n",
                "        f.write(f\"Mean PSNR (dB)         : {mean_psnr:.2f}\\n\")\n",
                "\n",
                "    print(f\"\\nMetrics summary saved to: {summary_json_path}\")\n",
                "    print(f\"Text summary saved to   : {summary_txt_path}\")\n",
                "    print(f\"Sample images saved to  : {OUTPUT_DIR} (up to {SAMPLE_LIMIT} frames)\")\n",
                "\n",
                "main()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Stage 1 Training: Coarse Restoration\n",
                "\n",
                "## Overview\n",
                "This notebook implements **Stage 1** of the two-stage training strategy for the raindrops removal model. \n",
                "\n",
                "### Goal\n",
                "The primary objective of Stage 1 is to train the model to perform **coarse restoration**. We focus solely on minimizing the pixel-wise difference between the predicted and clean images. This helps the model learn the basic structure and color correctness without being overwhelmed by complex perceptual metrics early on.\n",
                "\n",
                "### Key Features\n",
                "- **Model**: `MobileNetV3UNetConvLSTMVideo` (Encoder-Decoder with ConvLSTM bottleneck).\n",
                "- **Pretraining**: The encoder (`MobileNetV3`) uses ImageNet weights and is **frozen** to preserve robust feature extraction.\n",
                "- **Loss Function**: Pure **Pixel Loss** (Charbonnier Loss). Perceptual and structural losses are disabled (`beta=0`, `gamma=0`, `epsilon=0`).\n",
                "- **Strategy**: Per-frame training (T=1) to establish a solid baseline before temporal fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import time\n",
                "from pathlib import Path\n",
                "import torch\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "from torch.amp import autocast, GradScaler\n",
                "\n",
                "# Set Project Root\n",
                "current_dir = Path.cwd()\n",
                "if current_dir.name == 'training':\n",
                "    BASE = current_dir.parent\n",
                "else:\n",
                "    BASE = current_dir\n",
                "\n",
                "sys.path.insert(0, str(BASE))\n",
                "print(f'Project Root: {BASE}')\n",
                "\n",
                "from training.helpers.model import MobileNetV3UNetConvLSTMVideo\n",
                "from training.helpers.dataset import RainRemovalDataset\n",
                "from training.helpers.losses import CombinedVideoLoss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration\n",
                "\n",
                "We define the training hyperparameters here. Note the use of `EARLY_STOPPING_PATIENCE` to prevent overfitting if the validation loss plateaus.\n",
                "\n",
                "- **`BATCH_SIZE`**: 64 (Adjust based on VRAM).\n",
                "- **`MAX_EPOCHS`**: 50 (Upper limit, usually stops earlier).\n",
                "- **`LEARNING_RATE`**: 5e-5 (Conservative starting rate).\n",
                "- **`IMG_SIZE`**: 512x512 (Standard resolution for training patches)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths\n",
                "CLEAN_DATA = BASE / \"data\" / \"data_original\"\n",
                "RAINY_DATA = BASE / \"data\" / \"data_crapified_train\"\n",
                "CHECKPOINT_DIR = BASE / \"training\" / \"checkpoints\"\n",
                "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "# Resume from checkpoint flag\n",
                "RESUME_TRAINING = False\n",
                "RESUME_PATH = CHECKPOINT_DIR / \"stage1\" / \"latest_stage1.pth\"\n",
                "\n",
                "# Stage-1 Hyperparameters\n",
                "BATCH_SIZE = 64\n",
                "MAX_EPOCHS = 50\n",
                "LEARNING_RATE = 5e-5           # Reduced LR (many more updates now)\n",
                "FRAMES_PER_CLIP = 1           # T=1 so ConvLSTM has no temporal effect\n",
                "IMG_SIZE = (512, 512)          # Network input size (after crop/resize)\n",
                "NUM_WORKERS = 4\n",
                "EARLY_STOPPING_PATIENCE = 16\n",
                "SCHEDULER_PATIENCE = 7\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Main Training Loop\n",
                "\n",
                "The `main()` function orchestrates the entire workflow:\n",
                "\n",
                "1.  **Dataset Creation**: Initializes `RainRemovalDataset` with random cropping for data augmentation.\n",
                "2.  **Model Initialization**: Loads the architecture and freezes the encoder.\n",
                "3.  **Loss Setup**: Initializes `CombinedVideoLoss` with `alpha=1.0` (Pixel only).\n",
                "4.  **Training**: Iterates through epochs, computing gradients and updating weights using `Adam` optimizer and `GradScaler` for mixed precision.\n",
                "5.  **Validation**: Evaluates performance on the unseen validation set.\n",
                "6.  **Checkpointing**: Saves the `best_stage1.pth` whenever validation loss improves."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def main():\n",
                "    # ==================== DATASETS ====================\n",
                "    print(\"\\nCreating datasets (Stage 1, per-frame)...\")\n",
                "\n",
                "    # Train: every frame is a sample, random multi-scale square crops\n",
                "    train_dataset = RainRemovalDataset(\n",
                "        clean_base_dir=CLEAN_DATA,\n",
                "        rainy_base_dir=RAINY_DATA,\n",
                "        num_scenes=101,\n",
                "        frames_per_clip=FRAMES_PER_CLIP,\n",
                "        consecutive_frames=True,  # ignored in per_frame mode\n",
                "        img_size=IMG_SIZE,\n",
                "        split=\"train\",\n",
                "        train_ratio=0.8,\n",
                "        val_ratio=0.1,\n",
                "        per_frame=True,\n",
                "        random_crop=True,\n",
                "        crop_sizes=[256, 384, 512],\n",
                "        crop_probs=[0.15, 0.25, 0.60],\n",
                "    )\n",
                "\n",
                "    # Val: same per-frame logic + same crop behaviour for consistent distribution\n",
                "    val_dataset = RainRemovalDataset(\n",
                "        clean_base_dir=CLEAN_DATA,\n",
                "        rainy_base_dir=RAINY_DATA,\n",
                "        num_scenes=101,\n",
                "        frames_per_clip=FRAMES_PER_CLIP,\n",
                "        consecutive_frames=True,\n",
                "        img_size=IMG_SIZE,\n",
                "        split=\"val\",\n",
                "        train_ratio=0.8,\n",
                "        val_ratio=0.1,\n",
                "        per_frame=True,\n",
                "        random_crop=True,\n",
                "        crop_sizes=[256, 384, 512],\n",
                "        crop_probs=[0.15, 0.25, 0.60],\n",
                "    )\n",
                "\n",
                "    train_loader = DataLoader(\n",
                "        train_dataset,\n",
                "        batch_size=BATCH_SIZE,\n",
                "        shuffle=True,\n",
                "        num_workers=NUM_WORKERS,\n",
                "        pin_memory=True,\n",
                "    )\n",
                "    val_loader = DataLoader(\n",
                "        val_dataset,\n",
                "        batch_size=BATCH_SIZE,\n",
                "        shuffle=False,\n",
                "        num_workers=NUM_WORKERS,\n",
                "        pin_memory=True,\n",
                "    )\n",
                "\n",
                "    print(f\"Train samples:  {len(train_dataset)}  -> batches: {len(train_loader)}\")\n",
                "    print(f\"Val samples:    {len(val_dataset)}    -> batches: {len(val_loader)}\")\n",
                "\n",
                "    # ==================== MODEL ====================\n",
                "    print(\"\\nInitializing model...\")\n",
                "    model = MobileNetV3UNetConvLSTMVideo(\n",
                "        hidden_dim=96,\n",
                "        out_channels=3,\n",
                "        use_pretrained_encoder=True,\n",
                "        freeze_encoder=True,  # encoder weights + BN stats frozen\n",
                "    ).to(device)\n",
                "\n",
                "    # Initialize lazy layers with dummy input (T=1)\n",
                "    print(\"Initializing lazy layers...\")\n",
                "    with torch.no_grad():\n",
                "        dummy = torch.randn(1, FRAMES_PER_CLIP, 3, IMG_SIZE[0], IMG_SIZE[1]).to(device)\n",
                "        _ = model(dummy)\n",
                "        del dummy\n",
                "    print(\"✓ Lazy layers initialized\\n\")\n",
                "\n",
                "    model.print_param_summary()\n",
                "\n",
                "    # ==================== LOSS ====================\n",
                "    # Stage 1: train only with pixel loss\n",
                "    criterion = CombinedVideoLoss(\n",
                "        alpha=1.0,   # pixel (Charbonnier)\n",
                "        beta=0.0,    # SSIM\n",
                "        gamma=0.0,   # Edge\n",
                "        delta=0.0,   # Temporal\n",
                "        epsilon=0.0  # Perceptual\n",
                "    ).to(device)\n",
                "\n",
                "    print(\"Using CombinedVideoLoss (Stage 1 warmup):\")\n",
                "    print(f\"  alpha (pixel):      {criterion.alpha}\")\n",
                "    print(f\"  beta  (SSIM):       {criterion.beta}\")\n",
                "    print(f\"  gamma (edge):       {criterion.gamma}\")\n",
                "    print(f\"  delta (temporal):   {criterion.delta}\")\n",
                "    print(f\"  epsilon (percept.): {criterion.epsilon}\\n\")\n",
                "\n",
                "    # ==================== OPTIMIZER & SCHEDULER ====================\n",
                "    optimizer = optim.Adam(\n",
                "        filter(lambda p: p.requires_grad, model.parameters()),\n",
                "        lr=LEARNING_RATE,\n",
                "    )\n",
                "\n",
                "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
                "        optimizer,\n",
                "        mode=\"min\",\n",
                "        factor=0.5,\n",
                "        patience=SCHEDULER_PATIENCE,\n",
                "        threshold=1e-4,\n",
                "        threshold_mode=\"rel\",\n",
                "    )\n",
                "\n",
                "    scaler = GradScaler(\"cuda\")\n",
                "\n",
                "    best_val_loss = float(\"inf\")\n",
                "    train_losses = []\n",
                "    val_losses = []\n",
                "    patience_counter = 0\n",
                "    start_epoch = 0  # index in range(...)\n",
                "\n",
                "    if RESUME_TRAINING and RESUME_PATH.exists():\n",
                "        print(f\"\\n>>> Resuming from checkpoint: {RESUME_PATH}\\n\")\n",
                "        checkpoint = torch.load(RESUME_PATH, map_location=device)\n",
                "\n",
                "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
                "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
                "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
                "        scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
                "\n",
                "        # epoch in checkpoint is 1-based (epoch + 1), so we can resume from that index\n",
                "        start_epoch = checkpoint[\"epoch\"]  # next epoch index in range()\n",
                "        train_losses = checkpoint.get(\"train_losses\", [])\n",
                "        val_losses = checkpoint.get(\"val_losses\", [])\n",
                "\n",
                "        if val_losses:\n",
                "            best_val_loss = min(val_losses)\n",
                "        else:\n",
                "            best_val_loss = checkpoint.get(\"val_loss\", float(\"inf\"))\n",
                "\n",
                "        # optionally reset patience so early stopping doesn’t instantly trigger\n",
                "        patience_counter = 0\n",
                "\n",
                "        print(\n",
                "            f\"Resumed at epoch={checkpoint['epoch']} \"\n",
                "            f\"(best_val_loss={best_val_loss:.6f})\\n\"\n",
                "        )\n",
                "\n",
                "    print(\"=\" * 60)\n",
                "    print(\"STARTING STAGE 1 TRAINING (PER-FRAME)\")\n",
                "    print(\"=\" * 60)\n",
                "\n",
                "    for epoch in range(start_epoch, MAX_EPOCHS):\n",
                "        epoch_start = time.time()\n",
                "\n",
                "        # -------------------- TRAIN --------------------\n",
                "        model.train()\n",
                "        running_train_loss = 0.0\n",
                "\n",
                "        for batch_idx, (rainy, clean) in enumerate(train_loader):\n",
                "            rainy = rainy.to(device)   # (B, 1, 3, H, W)\n",
                "            clean = clean.to(device)   # (B, 1, 3, H, W)\n",
                "            optimizer.zero_grad()\n",
                "\n",
                "            with autocast(\"cuda\"):\n",
                "                output = model(rainy)\n",
                "                loss, loss_dict = criterion(output, clean)\n",
                "\n",
                "            scaler.scale(loss).backward()\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "\n",
                "            running_train_loss += loss.item()\n",
                "            if (batch_idx + 1) % 100 == 0:\n",
                "                print(\n",
                "                    f\"Epoch [{epoch + 1}/{MAX_EPOCHS}] \"\n",
                "                    f\"Batch [{batch_idx + 1}/{len(train_loader)}]\"\n",
                "                )\n",
                "                print(\n",
                "                    f\"  Total: {loss_dict['total']:.4f} | \"\n",
                "                    f\"Pixel: {loss_dict['pixel']:.4f} | \"\n",
                "                    f\"SSIM: {loss_dict['ssim']:.4f} | \"\n",
                "                    f\"Edge: {loss_dict['edge']:.4f} | \"\n",
                "                    f\"Temp: {loss_dict['temporal']:.4f} | \"\n",
                "                    f\"Perc: {loss_dict['perceptual']:.4f}\"\n",
                "                )\n",
                "\n",
                "        train_loss = running_train_loss / len(train_loader)\n",
                "        train_losses.append(train_loss)\n",
                "\n",
                "        # -------------------- VALIDATION --------------------\n",
                "        model.eval()\n",
                "        running_val_loss = 0.0\n",
                "\n",
                "        with torch.no_grad():\n",
                "            for rainy, clean in val_loader:\n",
                "                rainy = rainy.to(device)\n",
                "                clean = clean.to(device)\n",
                "                with autocast(\"cuda\"):\n",
                "                    output = model(rainy)\n",
                "                    loss, _ = criterion(output, clean)\n",
                "                running_val_loss += loss.item()\n",
                "\n",
                "        val_loss = running_val_loss / len(val_loader)\n",
                "        val_losses.append(val_loss)\n",
                "\n",
                "        scheduler.step(val_loss)\n",
                "\n",
                "        epoch_time = time.time() - epoch_start\n",
                "\n",
                "        print(\"\\n\" + \"-\" * 60)\n",
                "        print(f\"Epoch [{epoch + 1}/{MAX_EPOCHS}] completed\")\n",
                "        print(f\"Train Loss: {train_loss:.6f}\")\n",
                "        print(f\"Val Loss:   {val_loss:.6f}\")\n",
                "        print(f\"Time:       {epoch_time:.1f}s\")\n",
                "        print(f\"LR:         {optimizer.param_groups[0]['lr']:.6f}\")\n",
                "        print(f\"epsilon:    {criterion.epsilon:.4f}\")\n",
                "        print(\"-\" * 60 + \"\\n\")\n",
                "\n",
                "        # -------------------- CHECKPOINTING --------------------\n",
                "        checkpoint = {\n",
                "            \"epoch\": epoch + 1,\n",
                "            \"model_state_dict\": model.state_dict(),\n",
                "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
                "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
                "            \"scaler_state_dict\": scaler.state_dict(),\n",
                "            \"train_loss\": train_loss,\n",
                "            \"val_loss\": val_loss,\n",
                "            \"train_losses\": train_losses,\n",
                "            \"val_losses\": val_losses,\n",
                "        }\n",
                "\n",
                "        # latest\n",
                "        torch.save(checkpoint, CHECKPOINT_DIR / \"stage1\" / \"latest_stage1.pth\")\n",
                "\n",
                "        # best\n",
                "        if val_loss < best_val_loss:\n",
                "            best_val_loss = val_loss\n",
                "            patience_counter = 0\n",
                "            torch.save(checkpoint, CHECKPOINT_DIR / \"stage1\" / \"best_stage1.pth\")\n",
                "            print(f\"✓ New best Stage-1 model saved (val_loss={val_loss:.6f})\\n\")\n",
                "        else:\n",
                "            patience_counter += 1\n",
                "\n",
                "        # Occasional extra snapshot\n",
                "        if (epoch + 1) % 5 == 0:\n",
                "            torch.save(checkpoint, CHECKPOINT_DIR / \"stage1\" / f\"stage1_epoch_{epoch + 1}.pth\")\n",
                "\n",
                "        # Early stopping\n",
                "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
                "            print(\"Early stopping triggered (Stage 1).\")\n",
                "            break\n",
                "\n",
                "    print(\"=\" * 60)\n",
                "    print(\"STAGE 1 TRAINING COMPLETE\")\n",
                "    print(\"=\" * 60)\n",
                "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
                "    print(f\"Checkpoints saved in: {CHECKPOINT_DIR}\")\n",
                "\n",
                "main()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}